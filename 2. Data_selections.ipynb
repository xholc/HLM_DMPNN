{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJH5jjSI9VkhzS/7ktdkhW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Feature importance"],"metadata":{"id":"TCyQfWlpVip2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4TzZF3gsxp9"},"outputs":[],"source":["!pip install rfpimp"]},{"cell_type":"markdown","source":["## 2.1 Data selection by ML feature importance. After you choosing the top important features, you can apply statistical methods for filtering outliers of the dataset. (see 2.2 for details)  "],"metadata":{"id":"AiEktGKhOqcl"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","data=pd.read_csv('/content/eli_dupli_padel_D.csv')\n","\n","target_list=['Molecular Weight','AlogP','NumHDonors','TPSA','NumHAcceptors','FractionCSP3','NumRotatableBonds','LogP_WildmanCrippen']"],"metadata":{"id":"cy53ObPB7XWS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestRegressor\n","import rfpimp\n","rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n","tar=target_list\n","#print(tar)\n","x= data[tar]\n","y= data['STANDARD_VALUE_LN']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 8259)\n","rf.fit(x_train, y_train)\n","imp = rfpimp.importances(rf, x_test, y_test)\n","\n","y_pred_mlr=rf.predict(x_test)\n","meanAbErr = metrics.mean_absolute_error(y_test, y_pred_mlr)\n","meanSqErr = metrics.mean_squared_error(y_test, y_pred_mlr)\n","rootMeanSqErr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_mlr))\n","print('Mean Absolute Error:', meanAbErr)\n","print('Mean Square Error:', meanSqErr)\n","print('Root Mean Square Error:', rootMeanSqErr)"],"metadata":{"id":"yzizvaJY7vrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(6, 3))\n","\n","ax.barh(imp.index, imp['Importance'], height=0.8, facecolor='grey', alpha=0.8, edgecolor='k')\n","ax.set_xlabel('Importance score')\n","ax.set_title('Permutation feature importance')\n","\n","plt.gca().invert_yaxis()\n","\n","fig.tight_layout()"],"metadata":{"id":"0_wJKsd271S8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn import datasets, ensemble\n","from sklearn.inspection import permutation_importance\n","from sklearn.metrics import mean_squared_error\n","\n","tar=target_list\n","x= data[tar]\n","y= data['STANDARD_VALUE_LN']\n","X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=13)\n","\n","params = {\n","    \"n_estimators\": 500,\n","    \"max_depth\": 4,\n","    \"min_samples_split\": 5,\n","    \"learning_rate\": 0.01,\n","    \"loss\": \"squared_error\",\n","}\n","reg = ensemble.GradientBoostingRegressor(**params)\n","reg.fit(X_train, y_train)\n","\n","mse = mean_squared_error(y_test, reg.predict(X_test))\n","print(\"The mean squared error (MSE) on test set: {:.4f}\".format(mse))\n","\n","y_pred_mlr=reg.predict(X_test)\n","meanAbErr = metrics.mean_absolute_error(y_test, y_pred_mlr)\n","meanSqErr = metrics.mean_squared_error(y_test, y_pred_mlr)\n","rootMeanSqErr = np.sqrt(metrics.mean_squared_error(y_test, y_pred_mlr))\n","print('Mean Absolute Error:', meanAbErr)\n","print('Mean Square Error:', meanSqErr)\n","print('Root Mean Square Error:', rootMeanSqErr)"],"metadata":{"id":"L7pZE_p_8Ja0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_importance = reg.feature_importances_\n","sorted_idx = np.argsort(feature_importance)\n","pos = np.arange(sorted_idx.shape[0]) + 0.5\n","fig = plt.figure(figsize=(12, 6))\n","plt.subplot(1, 2, 1)\n","plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n","plt.yticks(pos, np.array(target_list)[sorted_idx])\n","plt.title(\"Feature Importance (MDI)\")\n","\n","result = permutation_importance(\n","    reg, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n",")\n","sorted_idx = result.importances_mean.argsort()\n","plt.subplot(1, 2, 2)\n","plt.boxplot(\n","    result.importances[sorted_idx].T,\n","    vert=False,\n","    labels=np.array(target_list)[sorted_idx],\n",")\n","plt.title(\"Permutation Importance (test set)\")\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"QntX4Sdk8OQc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.2 Data selection by descriptors (IQR method)"],"metadata":{"id":"6zpQLgFDVrQd"}},{"cell_type":"code","source":["from torch.cuda.memory import reset_max_memory_allocated\n","import numpy as np\n","test= pd.read_csv('/content/eli_dupli_padel_D.csv')\n","\n","Q1M = test['Molecular Weight'].quantile(0.25)\n","Q3M = test['Molecular Weight'].quantile(0.75)\n","IQRM = Q3M - Q1M\n","lowerM = Q1M - 1.5*IQRM\n","upperM = Q3M + 1.5*IQRM\n","# Create arrays of Boolean values indicating the outlier rows\n","upper_arrayM = np.where(test['Molecular Weight']>=upperM)[0]\n","lower_arrayM = np.where(test['Molecular Weight']<=lowerM)[0]\n","\n","# IQR method, see: https://medium.com/@pp1222001/outlier-detection-and-removal-using-the-iqr-method-6fab2954315d\n","Q1T = test['TPSA'].quantile(0.25)\n","Q3T = test['TPSA'].quantile(0.75)\n","IQRT = Q3T - Q1T\n","lowerT = Q1T - 1.5*IQRT\n","upperT = Q3T + 1.5*IQRT\n","upper_arrayT = np.where(test['TPSA']>=upperT)[0]\n","lower_arrayT = np.where(test['TPSA']<=lowerT)[0]\n","\n","Q1A = test['AlogP'].quantile(0.25)\n","Q3A = test['AlogP'].quantile(0.75)\n","IQRA = Q3A - Q1A\n","lowerA = Q1A - 1.5*IQRA\n","upperA = Q3A + 1.5*IQRA\n","upper_arrayA = np.where(test['AlogP']>=upperA)[0]\n","lower_arrayA = np.where(test['AlogP']<=lowerA)[0]"],"metadata":{"id":"zeu7ouW-Vxr1","colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"status":"error","timestamp":1707069797867,"user_tz":-480,"elapsed":6569,"user":{"displayName":"鍾岳辰","userId":"02693086127626364792"}},"outputId":"c49c6aed-84f8-442d-b227-0f3bcb7fba1a"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-2019d2d7a8b0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreset_max_memory_allocated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/eli_dupli_padel_D.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mQ1M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Molecular Weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}]},{"cell_type":"code","source":["up_rm_MT= np.union1d(upper_arrayM, upper_arrayT)\n","lw_rm_MT= np.union1d(lower_arrayM, lower_arrayT)\n","s2=np.union1d(up_rm_MT,lw_rm_MT)\n","# Removing the outliers\n","test.drop(index=s2, inplace=True)\n","test.to_csv('Qrm_MW_TPSA.csv', index=False)\n","\n","up_rm_MA= np.union1d(upper_arrayM, upper_arrayA)\n","lw_rm_MA= np.union1d(lower_arrayM, lower_arrayA)\n","s2=np.union1d(up_rm_MA,up_rm_MT)\n","s3=np.union1d(lw_rm_MA,lw_rm_MT)\n","s4=np.union1d(s2,s3)\n","# Removing the outliers\n","test.drop(index=s4, inplace=True)\n","test.to_csv('Qrm_MW_AlogP_TPSA.csv', index=False)"],"metadata":{"id":"_pRAhIZ3V8fd","executionInfo":{"status":"aborted","timestamp":1707069797868,"user_tz":-480,"elapsed":6,"user":{"displayName":"鍾岳辰","userId":"02693086127626364792"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.3 Data selection by N3/N7 ring"],"metadata":{"id":"MLmyT6duWmd_"}},{"cell_type":"code","source":["test= pd.read_csv('/content/rm_mol.csv')\n","# About N3/ N7 ring substrutures, please read thesis_R09945062\n","smiles, target= test['smiles'], test['half_life']\n","N3, N7= test['n3_ring'], test['n7_ring']\n","N3= N3.astype(np.bool)\n","N7= N7.astype(np.bool)\n","N37= np.logical_or(N3, N7)\n","smiles_rm_N3= smiles[~N3]\n","target_rm_N3= target[~N3]\n","smiles_rm_N7= smiles[~N7]\n","target_rm_N7= target[~N7]\n","smiles_rm_N37= smiles[~N37]\n","target_rm_N37= target[~N37]\n","\n","df = pd.DataFrame(list(zip(smiles_rm_N3, target_rm_N3)), columns =['smiles', 'half_life'])\n","df.to_csv('training_rm3.csv', index=False)\n","df2 = pd.DataFrame(list(zip(smiles_rm_N7, target_rm_N7)), columns =['smiles', 'half_life'])\n","df2.to_csv('training_rm7.csv', index=False)\n","df3 = pd.DataFrame(list(zip(smiles_rm_N37, target_rm_N37)), columns =['smiles', 'half_life'])\n","df3.to_csv('training_rm37.csv', index=False)"],"metadata":{"id":"9Lx1vbmYWrFf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.4 Data selection by wwl distance matrix"],"metadata":{"id":"N5cgVw_OXLNZ"}},{"cell_type":"code","source":["X= np.load('wasserstein_distance_matrix_it6.npy')\n","dis=[]\n","for i in X:\n","  d= np.sum(i)\n","  dis.append(d)\n","  #print(d)\n","flattened_distances= np.array(dis)\n","q1 = np.percentile(flattened_distances, 25)\n","q3 = np.percentile(flattened_distances, 75)\n","iqr = q3 - q1\n","lower_bound = q1 - 1.5 * iqr\n","upper_bound = q3 + 1.5 * iqr\n","\n","# Find the indices of outliers in the flattened distance matrix\n","outlier_indices = np.where((flattened_distances < lower_bound) | (flattened_distances > upper_bound))\n","print(outlier_indices)"],"metadata":{"id":"FyVqsJhaXP36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df= pd.read_csv('/content/eli_duplcate_hf.csv')\n","df2= df.drop(outlier_indices[0])\n","df2.to_csv('drop_by_q1q3.csv', index=False)"],"metadata":{"id":"jPIanqXUXdxM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.5 Even class training"],"metadata":{"id":"SbsAHTX1-v2y"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans, MiniBatchKMeans\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from imblearn.over_sampling import KMeansSMOTE, RandomOverSampler\n","from sklearn.datasets import make_blobs\n","\n","# https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\n","def over_dataset(file_path: str, out_file_name: str):\n","  # input needs to have class labels\n","  test_df= pd.read_csv(file_path)\n","  # Here we use cluster labels as classes labels to adapt the training technique of classification\n","  X, y= np.array(test_df['smiles']).reshape(-1, 1) , np.array(test_df['cluster_label']).reshape(-1, 1)\n","  # this will keep the original class distribution of dataset in training and testing set, respectivly\n","  sss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=1348)\n","\n","  train_index, test_index = next(sss.split(X, y))\n","  train_laebl= test_df['cluster_label'][train_index]\n","  test_laebl= test_df['cluster_label'][test_index]\n","  # get size of each class\n","  train_cluster_ids, train_cluster_sizes = np.unique(train_laebl, return_counts=True)\n","  test_cluster_ids, test_cluster_sizes = np.unique(test_laebl, return_counts=True)\n","  result_split= pd.DataFrame.from_dict({'cluster_label':train_cluster_ids, 'train_size_ratio':np.array(train_cluster_sizes/len(train_index)), 'test_size_ratio':np.array(test_cluster_sizes/len(test_index))})\n","  print(result_split)\n","  # 26 cluster in kmeans\n","  ros= RandomOverSampler(random_state=42)\n","  #In default setting, this RandomOverSampler will oversampling the minor class until size of all classes are the same\n","  X_res, y_res = ros.fit_resample(X[train_index], y[train_index])\n","  print('oversample size: ', len(X_res))\n","  cluster_ids, cluster_sizes = np.unique(y_res, return_counts=True)\n","  print(cluster_sizes)\n","  print(cluster_ids)\n","\n","  df_dict= dict(zip(test_df['smiles'], test_df['half_life']))\n","  tr_hf= np.array([df_dict[str(i[0])] for i in X_res])\n","  te_hf= np.array([df_dict[str(i[0])] for i in X[test_index]])\n","  ts=set([str(i[0]) for i in X_res])\n","  tes=set([str(i[0]) for i in X[test_index]])\n","  print('test smiles duplicate: ', ts.intersection(tes))\n","\n","  tr= pd.DataFrame()\n","  tr['smiles']= np.array([str(i[0]) for i in X_res])\n","  tr['half_life']= tr_hf\n","  tr['labels']= y_res\n","  print(tr.shape)\n","  #print(tr)\n","  te= pd.DataFrame()\n","  te['smiles']= np.array([str(i[0]) for i in X[test_index]])\n","  te['half_life']= te_hf\n","  te['labels']= y[test_index]\n","  print(te.shape)\n","  #print(te)\n","\n","  tr.to_csv(f'{out_file_name}_train.csv', index= False)\n","  te.to_csv(f'{out_file_name}_test.csv', index= False)"],"metadata":{"id":"8KtXyTwn-j2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["over_dataset('/content/kmean_df.csv', 'over_kmean')"],"metadata":{"id":"MeckZoDYyQc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.6 Scaffold analysis"],"metadata":{"id":"KQYxW5Xr_BGz"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from rdkit import Chem\n","from rdkit.Chem import Draw\n","from rdkit.Chem import rdFMCS\n","\n","df= pd.read_csv('/content/kmean_sus.csv', sep='\\t')\n","group= np.unique(df['group'])\n","\n","smi=[]\n","for i in group:\n","  print(i)\n","  s= df['smiles'][df['group']==i]\n","  smi.append({str(i):list(s)})\n","\n","smi8_10_1= smi[0]['8.1'].index('CCN(CC)CCCC(C)Nc1ccnc2cc(C(F)(F)F)ccc12')\n","smi8_10_2= smi[0]['8.1'].index('COc1cc2nc(Nc3ccc(F)cc3)nc(NCC3CCCO3)c2cc1OC')\n","print(smi8_10_1, smi8_10_2)\n","print(smi)\n","s1= smi[0]['8.1'].pop(smi8_10_1)\n","s2= smi[0]['8.1'].pop(smi8_10_2)\n","smi.insert(1, {'8_10':[s1,s2]})\n","smi.pop(3)"],"metadata":{"id":"ixSF957C_CxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["smi"],"metadata":{"id":"gdf9zyzN_ixG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 2.6.1 Find MCS without scaffold"],"metadata":{"id":"uKWDLnoSgdYE"}},{"cell_type":"code","source":["smart_temp=[]\n","smart_mol=[]\n","for i in range(len(smi)):\n","  print(str(smi[i].keys()))\n","  smis= list(smi[i].values())\n","  print(smis[0])\n","  mols = [Chem.MolFromSmiles(i) for i in smis[0]]\n","  #plain MCS\n","  res = rdFMCS.FindMCS(mols, atomCompare=rdFMCS.AtomCompare.CompareAny, bondCompare=rdFMCS.BondCompare.CompareAny).smartsString\n","  m = Chem.MolFromSmarts(res)\n","  smart_temp.append(res)\n","  smart_mol.append(m)\n","img = Draw.MolsToGridImage(smart_mol, subImgSize=(250, 250), molsPerRow=5)\n","img\n","with open(f'MCS.png','wb+') as outf:\n","  outf.write(png)"],"metadata":{"id":"FIWW-5YZ_m_y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2.6.2 Find MCS after getting scaffold"],"metadata":{"id":"DI_sFXCxgo6z"}},{"cell_type":"code","source":["core_temp_free=[]\n","core_mol_free=[]\n","for i in range(len(smi)):\n","  print(str(smi[i].keys()))\n","  smis= list(smi[i].values())\n","  print(smis[0])\n","  mols = [Chem.MolFromSmiles(i) for i in smis[0]]\n","  temp_mol=[]\n","  for j in mols:\n","    core = MurckoScaffold.GetScaffoldForMol(j)\n","    smi_temp= MurckoScaffold.MurckoScaffoldSmiles(mol=j)\n","    #temps.append(smi_temp)\n","    temp_mol.append(core)\n","  res = rdFMCS.FindMCS(temp_mol, atomCompare=rdFMCS.AtomCompare.CompareAny, bondCompare=rdFMCS.BondCompare.CompareAny).smartsString\n","  core_temp_free.append(res)\n","  m = Chem.MolFromSmarts(res)\n","  print(res)\n","  core_mol_free.append(m)\n","img_free = Draw.MolsToGridImage(core_mol_free, subImgSize=(250, 250), molsPerRow=5)\n","img_free\n","png = img_free.data\n","with open(f'MCS_under_scaffold.png','wb+') as outf:\n","  outf.write(png)"],"metadata":{"id":"mhjjiI6QAHEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#2.7 Frequency"],"metadata":{"id":"I24wgjzI_9sZ"}},{"cell_type":"code","source":["whole_smi= set(pd.read_csv('/content/eli_duplcate_hf.csv')['smiles'])\n","frq_free=[]\n","samp_free=[]\n","comp_mol=[]\n","for i in range(len(smi)):\n","  print(str(smi[i].keys()))\n","  smis= list(smi[i].values())\n","  gs= set(smis[0])\n","  print(len(gs))\n","  mols = whole_smi.difference(gs)\n","  print(len(mols))\n","  comp_mol.append(mols)\n","\n","for t in range(len(core_temp_free)):\n","  patt = Chem.MolFromSmarts(core_temp_free[t])\n","  rank=[]\n","  for i in comp_mol[t]:\n","    mol = Chem.MolFromSmiles(i)\n","    rk= mol.HasSubstructMatch(patt)\n","    rank.append(rk)\n","  rank_ids, rank_sizes = np.unique(rank, return_counts=True)\n","  frq_free.append(np.array(rank_sizes)/total)\n","  print(rank_ids)\n","  print(rank_sizes)\n","  samp_free.append(rank_sizes)\n","\n","print(frq_free)"],"metadata":{"id":"8hIq_-zA_5in"},"execution_count":null,"outputs":[]}]}